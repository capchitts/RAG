{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChitreshKaushik\\Documents\\RAG_UseCases\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001CC9E363B60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001CC9E1E8830>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some sub-questions to break down the complex query:\n",
      "\n",
      "1. **How does LangChain utilize memory in its applications?**  (Focuses on LangChain's memory capabilities)\n",
      "2. **What types of agents are supported by LangChain, and how are they implemented?** (Focuses on LangChain's agent functionality)\n",
      "3. **How does CrewAI handle memory management within its system?** (Focuses on CrewAI's memory approach)\n",
      "4. **What are the key differences in agent design and functionality between LangChain and CrewAI?** (Directly compares agent aspects of both) \n",
      "\n",
      "\n",
      "These sub-questions allow for more targeted document retrieval by isolating specific aspects of memory and agent usage in both LangChain and CrewAI. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eaeb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_qs_text = decomposition_chain.invoke({\"question\": \"How does LangChain use memory and agents compared to CrewAI?\"})\n",
    "sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f315b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are some sub-questions that break down the complex question:',\n",
       " \"**How does LangChain manage memory in its applications?** (Focuses on LangChain's specific memory mechanisms)\",\n",
       " \"**What types of agents can be built using LangChain, and how do they leverage memory?** (Explores LangChain's agent capabilities and memory integration)\",\n",
       " \"**How does CrewAI handle memory and agent functionality?** (Focuses on CrewAI's approach to these concepts)\",\n",
       " '**What are the key differences in memory management and agent design between LangChain and CrewAI?** (Directly compares the two systems)',\n",
       " \"These sub-questions allow for more focused document retrieval and a clearer understanding of the nuances between LangChain and CrewAI's approaches to memory and agents\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions that break down the complex query:\n",
      "A: Please provide the complex query you want to break down. I need the actual question to be able to generate sub-questions.  \n",
      "\n",
      "For example, you could say:\n",
      "\n",
      "\"Here is the complex query: **How can I use CrewAI to analyze a legal document and summarize the key findings?**  Break this down into sub-questions.\" \n",
      "\n",
      "\n",
      "Then I can help you formulate sub-questions! ðŸ˜Š \n",
      "\n",
      "\n",
      "Q: **What memory management techniques does LangChain employ?**  (This focuses on LangChain's specific approach)\n",
      "A: LangChain uses memory modules like **ConversationBufferMemory** and **ConversationSummaryMemory** to manage memory.  \n",
      "\n",
      "* **ConversationBufferMemory** allows the LLM to remember previous conversation turns. \n",
      "* **ConversationSummaryMemory** summarizes long interactions to keep the context within token limits. \n",
      "\n",
      "\n",
      "Let me know if you'd like more details on how these memory modules work! \n",
      "\n",
      "\n",
      "Q: **How does LangChain utilize agents for task completion?** (This focuses on LangChain's agent capabilities)\n",
      "A: LangChain utilizes agents to complete tasks by employing a **planner-executor model**. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. **Planning:** The agent, powered by an LLM, analyzes the given goal and devises a sequence of actions (tool invocations) needed to achieve it. This involves:\n",
      "    * **Dynamic Decision-Making:** The agent can adapt its plan based on the results of previous steps.\n",
      "    * **Branching Logic:** The agent can create alternative paths within its plan, choosing the most suitable one based on the situation.\n",
      "    * **Context-Aware Memory:** The agent remembers information from previous steps and uses it to inform its decisions in subsequent steps.\n",
      "\n",
      "2. **Execution:** The agent executes the planned sequence of actions. This often involves interacting with external tools like:\n",
      "    * **Web Search:** Retrieving information from the internet.\n",
      "    * **Calculators:** Performing mathematical calculations.\n",
      "    * **Code Execution:** Running code snippets.\n",
      "\n",
      "Essentially, LangChain agents act as intelligent intermediaries, leveraging LLMs to reason and plan, and then using tools to carry out the necessary actions, ultimately accomplishing complex multi-step tasks. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q: **What are the key memory features of CrewAI?** (This focuses on CrewAI's memory capabilities)\n",
      "A: The provided context doesn't explicitly describe CrewAI's memory features. \n",
      "\n",
      "It emphasizes CrewAI's strengths in multi-step workflows and its ability to:\n",
      "\n",
      "* **Define agents with purposes, goals, and tools:** This suggests agents might have internal memory to track their progress and utilize tools effectively.\n",
      "* **Ensure agents stay on task and contribute meaningfully:** This implies some form of memory to maintain context and understand the overall workflow objective.\n",
      "\n",
      "However, without further information, we can't specify the exact nature or capabilities of CrewAI's memory. \n",
      "\n",
      "\n",
      "It's possible CrewAI uses:\n",
      "\n",
      "* **Local memory within each agent:**  Agents store information relevant to their specific tasks.\n",
      "* **Shared memory:** Agents can access and modify a common pool of information.\n",
      "* **External memory:**  Data is stored outside the agents and accessed as needed.\n",
      "\n",
      "\n",
      "You would need to consult CrewAI's documentation or technical specifications for a detailed explanation of its memory features. \n",
      "\n",
      "\n",
      "Q: **How does CrewAI leverage agents in its workflow?** (This focuses on CrewAI's agent capabilities)\n",
      "A: CrewAI leverages agents by:\n",
      "\n",
      "* **Defining roles:** Each agent has a specific role, like researcher, planner, or executor, ensuring clear responsibilities within the workflow.\n",
      "* **Setting goals and purposes:** Agents are equipped with a defined purpose and goal, keeping them focused and contributing to the overall crew objective.\n",
      "* **Providing tools:** Agents are given the necessary tools to accomplish their tasks effectively.\n",
      "* **Enabling semi-independent operation:** Agents work autonomously within the defined framework, collaborating with others to achieve the crew's goals. \n",
      "* **Structuring workflows:** Agents form structured workflows, allowing for a more organized and efficient approach to completing complex tasks. \n",
      "\n",
      "\n",
      "Essentially, CrewAI uses agents as specialized, collaborating units within a larger system, each contributing their unique capabilities to achieve a common goal. \n",
      "\n",
      "\n",
      "Q: These sub-questions allow for more targeted document retrieval by isolating specific aspects of LangChain and CrewAI's memory and agent functionalities\n",
      "A: The provided context heavily emphasizes the functionalities of **LangChain** and its integration with **CrewAI**.  \n",
      "\n",
      "Specifically, it highlights:\n",
      "\n",
      "* **Hybrid Retrieval:** LangChain's ability to combine keyword-based and embedding-based retrieval methods for more comprehensive search results.\n",
      "* **Integration with CrewAI:** CrewAI's compatibility with LangChain, allowing for collaborative systems where LangChain handles retrieval and tool management while CrewAI focuses on role-based collaboration.\n",
      "* **Vector Database Integration:** LangChain's seamless integration with vector databases, facilitating semantic search within large document corpora, which is crucial for Retrieval-Augmented Generation (RAG).\n",
      "\n",
      "\n",
      "The statement \"These sub-questions allow for more targeted document retrieval by isolating specific aspects of LangChain and CrewAI's memory and agent functionalities\" suggests that these sub-questions are designed to delve deeper into these specific features. \n",
      "\n",
      "For example, sub-questions might focus on:\n",
      "\n",
      "* **How does LangChain's hybrid retrieval mechanism work in practice?**\n",
      "* **What types of role-based collaboration scenarios are possible with the integration of LangChain and CrewAI?**\n",
      "* **What are the benefits of using a vector database for semantic search within LangChain?**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions or need further clarification.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f7bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
