{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3679b0",
   "metadata": {},
   "source": [
    "### ğŸ§  What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "â€œIs this clear, complete, and accurate?â€\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, weâ€™ll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f3ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc7a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(r\"research_notes.txt\",encoding=\"utf-8\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2246d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. State Definition\n",
    "# -------------------------\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question: str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\"\n",
    "    reflection: str = \"\"\n",
    "    revised: bool = False\n",
    "    attempts: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a454cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 3. Nodes\n",
    "# -------------------------\n",
    "\n",
    "# a. Retrieve\n",
    "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\": docs})\n",
    "\n",
    "# b. Generate Answer\n",
    "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "                Use the following context to answer the question:\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question:\n",
    "                {state.question}\n",
    "            \"\"\"\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de27da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflect\n",
    "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            Reflect on the following answer to see if it fully addresses the question. \n",
    "            State YES if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "            Question: {state.question}\n",
    "\n",
    "            Answer: {state.answer}\n",
    "\n",
    "            Respond like:\n",
    "            Reflection: YES or NO\n",
    "            Explanation: ...\n",
    "            \"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a07cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Finalizer\n",
    "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eafcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4. LangGraph DAG\n",
    "# -------------------------\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    # direct to END state if not revised or revised more than 2 times\n",
    "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c3ed5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Final Answer:\n",
      " **Transformer variants that have already been moved into productionâ€‘grade use (as reported in the Julyâ€¯2024 log)**  \n",
      "\n",
      "| Variant | How it is being used in production / deployment target | Key deploymentâ€‘relevant notes |\n",
      "|---------|--------------------------------------------------------|--------------------------------|\n",
      "| **EfficientFormer** | Deployed on edge devices (Raspberryâ€¯Piâ€¯4) for imageâ€‘classification tasks (TinyImageNet). | Fits the 290â€¯MB memory budget at batchâ€‘sizeâ€¯16; quantized int8 runs with negligible accuracy loss (Topâ€‘1â€¯â‰ˆâ€¯92.4â€¯%). |\n",
      "| **Longformer** | Serves a customerâ€‘supportâ€‘log retrieval/analysis service where documents can be up to 8â€¯k tokens. | Streaming latency is >1.2â€¯s; the team is testing chunkâ€‘based hybrid attention to bring that down, indicating it is already in a productionâ€‘oriented pipeline. |\n",
      "| **TinyBERT** | Powers a supportâ€‘ticket priorityâ€‘tagging classifier. | Achieves 87â€¯%â€¯F1; fineâ€‘tuned with a 2â€‘layer FFN adapter for domain transfer, and is running in the live ticketâ€‘routing system. |\n",
      "| **LLaMAâ€‘2 + FlashAttentionâ€‘2** | Used for generativeâ€‘AI services that need longâ€‘context generation (e.g., chat or report drafting). | FlashAttentionâ€‘2 integration cuts contextâ€‘window latency by ~50â€¯% and is the version shipped to the production inference fleet. |\n",
      "| **(Optional) Reformer** | Mentioned only in the â€œtrainingâ€‘issuesâ€ section; no clear production rollout yet. | Because of bucketâ€‘collision loss spikes and sparseâ€‘gradient instability, it is still experimental and not listed as a production model. |\n",
      "\n",
      "**In short:** the production stack currently includes **EfficientFormer, Longformer, TinyBERT, and the LLaMAâ€‘2 model accelerated with FlashAttentionâ€‘2**. Reformer is still in the research/experiment phase and has not been promoted to a production deployment.\n",
      "\n",
      "ğŸ” Reflection Log:\n",
      " Reflection: NO  \n",
      "Explanation: The question asks broadly for â€œthe transformer variants in production deployments.â€ A complete answer should enumerate the major transformer families that are actually deployed in realâ€‘world systems (e.g., BERT/RoBERTa, GPTâ€‘2/3/4, T5, XLNet, DistilBERT, etc.), possibly with brief notes on typical useâ€‘cases or deployment environments. The provided answer lists only a handful of niche models (EfficientFormer, Longformer, TinyBERT, LLaMAâ€‘2â€¯+â€¯FlashAttentionâ€‘2) and treats Reformer as â€œoptional.â€ It omits many widelyâ€‘used production variants and does not explain the selection criteria or give a comprehensive overview. Consequently, the answer does not fully address the question.\n",
      "ğŸ”„ Total Attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformer variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nğŸ§  Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nğŸ” Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"ğŸ”„ Total Attempts:\", result[\"attempts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
